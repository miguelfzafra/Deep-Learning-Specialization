# 2. Improving Deep Neural Networks

## Content

### Week 1 - Practical aspects of Deep Learning

**Theory**

* Setting up a machine learning application: train/dev/test sets, bias/variance.
* Basic recipe for machine learning.
* Regularization: L2 and dropout. Other regularization methods.
* Optimization: normalizing inputs, vanishing/exploding gradients, weight initialization, gradient checking.

**Practice**

* Initialization: zero, random and He initialization.
* Regularization: L2 and dropout.
* Gradient Checking: implementation.


### Week 2 - Optimization algorithms

**Theory**

* Mini-batch gradient descent.
* Exponentially weighted averages. Bias correction.
* Gradient descent with momentum.
* RMSprop and Adam optimization algorithms.
* Learning rate decay.
* Local optima and plateaus.


**Practice**

* Optimization methods: GD, mini-batch GD, GD with momentum, Adam. 

### Week 3 - Hyperparameter Tuning, Batch Normalization and Programming Frameworks

**Theory**

* Hyperparameter tuning process. Picking an appropiate scale. Pandas vs. Caviar.
* Batch normalization: motivation, intuition and implementation. Batch norm at test time.
* Multi-class classification.
* Introduction to programming frameworks.


**Practice**

* TensorFlow example.
* TensorFlow implementation of a 3-layer NN.
